{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "01beebdb",
      "metadata": {
        "id": "01beebdb"
      },
      "source": [
        "# EE 344 — Assignment 4: Fake News Classification\n",
        "\n",
        "In this assignment, you will classify news articles as **fake vs real** using **text features**.\n",
        "Your tasks for this assignment are as follows:\n",
        "\n",
        "1. Learn how to extract text features by vectorizing textual inputs using **CountVectorizer (Bag-of-Words)**.\n",
        "2. Implement **7 classifiers**: Logistic Regression, Perceptron, Linear SVM (LinearSVC), Multinomial Naive Bayes, KNN, Decision Tree, and Random Forest.\n",
        "3. Evaluate **train and test** performance using **accuracy, precision, recall, and F1-score**.\n",
        "4. Provide brief answers to discussion questions about (i) the text feature extraction method you implemented and (ii) the effect of using two different KNN distance choices (**Euclidean vs cosine**).\n",
        "\n",
        "\n",
        "## Submission guidelines\n",
        "- Complete all **[TODO]** blocks in this notebook.\n",
        "- Push the finished notebook to your GitHub repository.\n",
        "- Submit the GitHub link on the Canvas submission page.\n",
        "\n",
        "\n",
        "**Dataset source (for reference only):**  \n",
        "Do **not** download data from the link below. Use the provided `evaluation.csv` file that comes with this assignment.\n",
        "#### https://www.kaggle.com/datasets/aadyasingh55/fake-news-classification\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "df0944b7",
      "metadata": {
        "id": "df0944b7"
      },
      "source": [
        "## Setup\n",
        "Run the next cell to import libraries and define helper functions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "125eae54",
      "metadata": {
        "id": "125eae54"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression, Perceptron\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "\n",
        "# Reproducibility\n",
        "RANDOM_STATE = 42\n",
        "\n",
        "def metrics(y_true, y_pred):\n",
        "    \"\"\"Return (accuracy, precision, recall, f1).\"\"\"\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "\n",
        "    # Use 'binary' for binary classification, otherwise fallback to macro.\n",
        "    avg = \"binary\" if len(np.unique(y_true)) == 2 else \"macro\"\n",
        "\n",
        "    prec, rec, f1, _ = precision_recall_fscore_support(\n",
        "        y_true, y_pred, average=avg, zero_division=0\n",
        "    )\n",
        "    return acc, prec, rec, f1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c83ee37a",
      "metadata": {
        "id": "c83ee37a"
      },
      "source": [
        "## Load data\n",
        "\n",
        "Put the dataset file in the same folder as this notebook (recommended), or provide an absolute path.\n",
        "\n",
        "This dataset uses **semicolon-separated** fields and can contain extra semicolons inside the text.\n",
        "So we use a custom loader that safely reconstructs the text column.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "ce5700b1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ce5700b1",
        "outputId": "48c00802-6f73-46d2-b614-8315b9a45501"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset: (7815, 4)\n",
            "Label distribution:\n",
            " label\n",
            "1    4185\n",
            "0    3630\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# === Data path ===\n",
        "DATA_PATH = \"evaluation.csv\"\n",
        "\n",
        "def load_semicolon_dataset(path):\n",
        "    \"\"\"\n",
        "    Handles lines like:\n",
        "    ;title;text;label\n",
        "    0;some title;some text that may contain ; ; ; ;0\n",
        "    \"\"\"\n",
        "    rows = []\n",
        "    with open(path, \"r\", encoding=\"utf-8\", errors=\"replace\") as f:\n",
        "        _ = f.readline()  # header\n",
        "        for line in f:\n",
        "            line = line.rstrip(\"\\n\")\n",
        "            if not line:\n",
        "                continue\n",
        "            parts = line.split(\",\")\n",
        "            if len(parts) < 4:\n",
        "                continue\n",
        "\n",
        "            idx = parts[0]\n",
        "            title = parts[1]\n",
        "            label = parts[-1]\n",
        "            text = \";\".join(parts[2:-1])  # re-join any extra ';' inside text\n",
        "            rows.append((idx, title, text, label))\n",
        "\n",
        "    df = pd.DataFrame(rows, columns=[\"id\", \"title\", \"text\", \"label\"])\n",
        "    df[\"label\"] = pd.to_numeric(df[\"label\"], errors=\"coerce\")\n",
        "    df = df.dropna(subset=[\"label\"]).reset_index(drop=True)\n",
        "    df[\"label\"] = df[\"label\"].astype(int)\n",
        "    return df\n",
        "\n",
        "df = load_semicolon_dataset(DATA_PATH)\n",
        "print(\"Dataset:\", df.shape)\n",
        "print(\"Label distribution:\\n\", df[\"label\"].value_counts())\n",
        "\n",
        "# Combine title + text into one string per document\n",
        "docs = (df[\"title\"].fillna(\"\") + \" \" + df[\"text\"].fillna(\"\")).astype(str).tolist()\n",
        "y = df[\"label\"].values\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b6df6766",
      "metadata": {
        "id": "b6df6766"
      },
      "source": [
        "## Train/test split\n",
        "\n",
        "We keep a standard **80/20** split with stratification (preserves label ratio).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "d3ca0b1a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d3ca0b1a",
        "outputId": "a9fad0e6-d594-4d3e-8913-09d54b46d153"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train docs: 6252 Test docs: 1563\n"
          ]
        }
      ],
      "source": [
        "X_train_text, X_test_text, y_train, y_test = train_test_split(\n",
        "    docs, y,\n",
        "    test_size=0.20,\n",
        "    random_state=RANDOM_STATE,\n",
        "    stratify=y\n",
        ")\n",
        "\n",
        "print(\"Train docs:\", len(X_train_text), \"Test docs:\", len(X_test_text))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "39543025",
      "metadata": {
        "id": "39543025"
      },
      "source": [
        "## Case Study: Bag-of-Words Features (CountVectorizer)\n",
        "\n",
        "We need to convert text into numeric features before we can train ML models.\n",
        "\n",
        "**CountVectorizer** builds a vocabulary from the **training set** and represents each document as a vector of **counts** (one entry per vocabulary term).\n",
        "\n",
        "We will use:\n",
        "$$\n",
        "\\texttt{CountVectorizer(}\n",
        "\\texttt{lowercase=True, stopwords=\"english\", ngramrange=(1,2),}\n",
        "$$\n",
        "$$\n",
        "\\texttt{ mindf=2, maxdf=0.9, maxfeatures=10000)}\n",
        "$$\n",
        "\n",
        "**What each setting means (briefly):**\n",
        "- `lowercase=True`: convert text to lowercase before building features.\n",
        "- `stop_words=\"english\"`: remove a predefined list of common English words.\n",
        "- `ngram_range=(1,2)`: allow 1-word features and 2-word features (bigrams).\n",
        "- `min_df=2`: keep a term only if it appears in at least 2 training documents.\n",
        "- `max_df=0.9`: drop a term if it appears in more than 90% of training documents.\n",
        "- `max_features=10000`: cap the vocabulary size at 10,000 terms (after filtering).\n",
        "\n",
        "### Tiny example (just to see what it does)\n",
        "\n",
        "We will build features from 3 short documents and look at the counts.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "aaee0c65",
      "metadata": {
        "id": "aaee0c65"
      },
      "outputs": [],
      "source": [
        "# CountVectorizer docs (read this once before TODO 1):\n",
        "# https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "b5a2cb96",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b5a2cb96",
        "outputId": "cde73aa0-b478-4429-830f-55e84cc16d62"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Toy vocab: ['fake', 'fake news', 'news', 'news spreads', 'spreads']\n",
            "Toy counts (rows = docs):\n",
            " [[1 1 1 1 1]\n",
            " [1 1 1 1 1]\n",
            " [0 0 1 1 1]]\n"
          ]
        }
      ],
      "source": [
        "toy_docs = [\n",
        "    \"The FAKE news spreads fast\",\n",
        "    \"Fake news spreads\",\n",
        "    \"Real news spreads\",\n",
        "]\n",
        "\n",
        "toy_vec = CountVectorizer(\n",
        "    lowercase=True,\n",
        "    stop_words=\"english\",\n",
        "    ngram_range=(1, 2),\n",
        "    min_df=2\n",
        ")\n",
        "\n",
        "toy_X = toy_vec.fit_transform(toy_docs)\n",
        "\n",
        "print(\"Toy vocab:\", list(toy_vec.get_feature_names_out()))\n",
        "print(\"Toy counts (rows = docs):\\n\", toy_X.toarray())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "081e3f66",
      "metadata": {
        "id": "081e3f66"
      },
      "source": [
        "## Build Bag-of-Words features\n",
        "\n",
        "Goal:\n",
        "1. Create the `CountVectorizer` using the exact settings below.\n",
        "2. Fit on the training text only.\n",
        "3. Transform both train and test text into sparse Bag-of-Words features.\n",
        "\n",
        "Notes:\n",
        "- `fit_transform` on train, then `transform` on test.\n",
        "- The output is a **sparse matrix** (CSR). That is normal for text features.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "a6a1edcc",
      "metadata": {
        "id": "a6a1edcc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BoW shapes: (6252, 10000) (1563, 10000)\n"
          ]
        }
      ],
      "source": [
        "# --- Bag-of-Words settings ---\n",
        "MAX_FEATURES = 10000\n",
        "NGRAM_RANGE = (1, 2)\n",
        "\n",
        "## [ TODO 1 ]\n",
        "# 1) Create `vectorizer` using CountVectorizer with:\n",
        "#    lowercase=True\n",
        "#    stop_words=\"english\"\n",
        "#    ngram_range=NGRAM_RANGE\n",
        "#    min_df=2\n",
        "#    max_df=0.9\n",
        "#    max_features=MAX_FEATURES\n",
        "#\n",
        "# 2) Fit the vectorizer on the training text, then use it to transform:\n",
        "#    - the training text into BoW features\n",
        "#    - the test text into BoW features\n",
        "#\n",
        "# (Reminder: fit on train only; do NOT fit on test.)\n",
        "#\n",
        "# Print the BoW shapes.\n",
        "vectorizer = CountVectorizer(\n",
        "    lowercase=True,\n",
        "    stop_words=\"english\",\n",
        "    ngram_range=NGRAM_RANGE,\n",
        "    min_df=2,\n",
        "    max_df=0.9,\n",
        "    max_features=MAX_FEATURES\n",
        ")\n",
        "X_train_bow = vectorizer.fit_transform(X_train_text)\n",
        "X_test_bow = vectorizer.fit_transform(X_test_text)\n",
        "\n",
        "print(\"BoW shapes:\", X_train_bow.shape, X_test_bow.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5127e270",
      "metadata": {
        "id": "5127e270"
      },
      "source": [
        "## Models\n",
        "\n",
        "Create **7 classifiers** using the exact hyperparameters below.\n",
        "\n",
        "**Important:** For KNN in this notebook, start with **Euclidean distance**.\n",
        "\n",
        "Models to implement:\n",
        "- Logistic Regression: `solver=\"saga\"`, `max_iter=2000`, `n_jobs=-1`, `random_state=42`\n",
        "- Perceptron: `max_iter=1000`, `tol=1e-3`, `random_state=42`\n",
        "- SVM (LinearSVC): `random_state=42`\n",
        "- Naive Bayes (MultinomialNB): `alpha=1.0`\n",
        "- KNN (Euclidean): `n_neighbors=7`, `metric=\"euclidean\"`, `n_jobs=-1`\n",
        "- Decision Tree: `max_depth=40`, `random_state=42`\n",
        "- Random Forest: `n_estimators=300`, `random_state=42`, `n_jobs=-1`\n",
        "\n",
        "Put them in a dictionary named `models`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "78772066",
      "metadata": {
        "id": "78772066"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'Logistic Regression': LogisticRegression(max_iter=2000, n_jobs=-1, random_state=42, solver='saga'),\n",
              " 'Perceptron': Perceptron(random_state=42),\n",
              " 'SVM (LinearSVC)': LinearSVC(random_state=42),\n",
              " 'Naive Bayes (MultinomialNB)': MultinomialNB(),\n",
              " 'KNN (euclidian)': KNeighborsClassifier(metric='euclidean', n_jobs=-1, n_neighbors=7),\n",
              " 'Decision Tree': DecisionTreeClassifier(max_depth=40, random_state=42),\n",
              " 'Random Forest': RandomForestClassifier(n_estimators=300, n_jobs=-1, random_state=42)}"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "## [ TODO 2 ]\n",
        "# Build the `models` dictionary using the exact parameters above.\n",
        "def weighted_metrics(y_true, y_pred):\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "    prec, rec, f1, _ = precision_recall_fscore_support(\n",
        "        y_true, y_pred, average=\"weighted\", zero_division=0\n",
        "    )\n",
        "    return acc, prec, rec, f1\n",
        "\n",
        "models = {\n",
        "    \"Logistic Regression\": LogisticRegression(solver=\"saga\", max_iter=2000, n_jobs=-1, random_state=42),\n",
        "    \"Perceptron\": Perceptron(max_iter=1000, tol=1e-3, random_state=42),\n",
        "    \"SVM (LinearSVC)\": LinearSVC(random_state=42),\n",
        "    \"Naive Bayes (MultinomialNB)\": MultinomialNB(alpha=1.0),\n",
        "    \"KNN (euclidian)\": KNeighborsClassifier(n_neighbors=7, metric=\"euclidean\", n_jobs=-1),\n",
        "    \"Decision Tree\": DecisionTreeClassifier(random_state=42, max_depth=40),\n",
        "    \"Random Forest\": RandomForestClassifier(n_estimators=300, random_state=42, n_jobs=-1),\n",
        "}\n",
        "\n",
        "models"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c1adacab",
      "metadata": {
        "id": "c1adacab"
      },
      "source": [
        "## Train + evaluate\n",
        "\n",
        "We will evaluate each model on:\n",
        "- **Training set**\n",
        "- **Test set**\n",
        "\n",
        "Metrics:\n",
        "- Accuracy\n",
        "- Precision\n",
        "- Recall\n",
        "- F1\n",
        "\n",
        "We will print a table sorted by **Test F1**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "10fdf1af",
      "metadata": {
        "id": "10fdf1af"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Results (sorted by Test F1) ===\n",
            "                      Model Train Acc Train Prec Train Rec Train F1 Test Acc Test Prec Test Rec Test F1\n",
            "              Random Forest    1.0000     1.0000    1.0000   1.0000   0.9949    0.9949   0.9949  0.9949\n",
            "              Decision Tree    1.0000     1.0000    1.0000   1.0000   0.9904    0.9904   0.9904  0.9904\n",
            "        Logistic Regression    0.9989     0.9989    0.9989   0.9989   0.9872    0.9872   0.9872  0.9872\n",
            "            SVM (LinearSVC)    1.0000     1.0000    1.0000   1.0000   0.9853    0.9853   0.9853  0.9853\n",
            "                 Perceptron    1.0000     1.0000    1.0000   1.0000   0.9808    0.9808   0.9808  0.9808\n",
            "Naive Bayes (MultinomialNB)    0.9624     0.9625    0.9624   0.9624   0.9610    0.9610   0.9610  0.9610\n",
            "            KNN (euclidian)    0.7844     0.8071    0.7844   0.7829   0.7364    0.7666   0.7364  0.7330\n"
          ]
        }
      ],
      "source": [
        "## [ TODO 3 ]\n",
        "# Write a loop that:\n",
        "# 1) fits each model on X_train_bow, y_train\n",
        "# 2) predicts on train and test\n",
        "# 3) computes (acc, prec, rec, f1) using metrics(...)\n",
        "# 4) stores results in a list\n",
        "# 5) prints a DataFrame sorted by Test F1 (descending)\n",
        "#\n",
        "# Use the exact column names below.\n",
        "\n",
        "results = []\n",
        "best = {\"name\": None, \"pipe\": None, \"test_f1\": -1.0}\n",
        "\n",
        "for name, clf in models.items():\n",
        "    pipe = Pipeline(steps=[\n",
        "        (\"Vectorizer\", vectorizer),\n",
        "        (\"Classifier\", clf),\n",
        "    ])\n",
        "\n",
        "    pipe.fit(X_train_text, y_train)\n",
        "\n",
        "    yhat_tr = pipe.predict(X_train_text)\n",
        "    yhat_te = pipe.predict(X_test_text)\n",
        "\n",
        "    tr = weighted_metrics(y_train, yhat_tr)\n",
        "    te = weighted_metrics(y_test, yhat_te)\n",
        "\n",
        "    results.append([name, *tr, *te])\n",
        "\n",
        "    if te[3] > best[\"test_f1\"]:\n",
        "        best[\"name\"] = name\n",
        "        best[\"pipe\"] = pipe\n",
        "        best[\"test_f1\"] = te[3]\n",
        "\n",
        "cols = [\n",
        "    \"Model\",\n",
        "    \"Train Acc\", \"Train Prec\", \"Train Rec\", \"Train F1\",\n",
        "    \"Test Acc\", \"Test Prec\", \"Test Rec\", \"Test F1\",\n",
        "]\n",
        "\n",
        "out = pd.DataFrame(results, columns=cols).sort_values(\"Test F1\", ascending=False).reset_index(drop=True)\n",
        "\n",
        "pd.set_option(\"display.max_colwidth\", 80)\n",
        "print(\"\\n=== Results (sorted by Test F1) ===\")\n",
        "print(out.to_string(index=False, formatters={\n",
        "    \"Train Acc\": \"{:.4f}\".format,\n",
        "    \"Train Prec\": \"{:.4f}\".format,\n",
        "    \"Train Rec\": \"{:.4f}\".format,\n",
        "    \"Train F1\": \"{:.4f}\".format,\n",
        "    \"Test Acc\": \"{:.4f}\".format,\n",
        "    \"Test Prec\": \"{:.4f}\".format,\n",
        "    \"Test Rec\": \"{:.4f}\".format,\n",
        "    \"Test F1\": \"{:.4f}\".format,\n",
        "}))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "51e27447",
      "metadata": {
        "id": "51e27447"
      },
      "source": [
        "## Cosine distance for KNN\n",
        "\n",
        "With Bag-of-Words, each document becomes a long vector of word counts (mostly zeros).  \n",
        "To compare two documents, we need a way to measure how “close” two vectors are.\n",
        "\n",
        "Two common choices:\n",
        "\n",
        "- **Euclidean distance**: straight-line distance between two vectors.\n",
        "- **Cosine distance**: based on the angle between two vectors (uses cosine similarity under the hood).\n",
        "\n",
        "In scikit-learn, KNN uses a **distance**. Cosine distance is:\n",
        "$$\n",
        "d_{\\text{cosine}}(x, z) \\;=\\; 1 - \\cos(x, z)\n",
        "\\;=\\; 1 - \\frac{x^\\top z}{\\|x\\|_2 \\,\\|z\\|_2}\n",
        "$$\n",
        "\n",
        "(where $\\cos(x,z)$ is cosine similarity).\n",
        "\n",
        "### Tiny numeric example (no text, just vectors)\n",
        "\n",
        "Let:\n",
        "- $x = [1, 1]$\n",
        "- $z_1 = [2, 2]$  (same direction as $x$, just “bigger”)\n",
        "- $z_2 = [2, 0]$  (different direction)\n",
        "\n",
        "**Euclidean distances**\n",
        "$$\n",
        "\\|x - z_1\\|_2 = \\sqrt{(1-2)^2 + (1-2)^2} = \\sqrt{2}\n",
        "$$\n",
        "$$\n",
        "\\|x - z_2\\|_2 = \\sqrt{(1-2)^2 + (1-0)^2} = \\sqrt{2}\n",
        "$$\n",
        "So Euclidean says $z_1$ and $z_2$ are equally far from $x$ here.\n",
        "\n",
        "**Cosine distances**\n",
        "$$\n",
        "\\cos(x, z_1) = \\frac{1\\cdot 2 + 1\\cdot 2}{\\sqrt{2}\\cdot \\sqrt{8}} = 1\n",
        "\\Rightarrow d_{\\text{cosine}}(x, z_1)=0\n",
        "$$\n",
        "$$\n",
        "\\cos(x, z_2) = \\frac{1\\cdot 2 + 1\\cdot 0}{\\sqrt{2}\\cdot 2} \\approx 0.707\n",
        "\\Rightarrow d_{\\text{cosine}}(x, z_2)\\approx 0.293\n",
        "$$\n",
        "So cosine says $z_1$ is closer to $x$ than $z_2$.\n",
        "\n",
        "### What you will do\n",
        "\n",
        "Keep everything the same, but change your KNN metric from `\"euclidean\"` to `\"cosine\"`, then re-run your evaluation and compare results.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "04779264",
      "metadata": {
        "id": "04779264"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Results (sorted by Test F1) ===\n",
            "          Model Train Acc Train Prec Train Rec Train F1 Test Acc Test Prec Test Rec Test F1\n",
            "      KNN (cos)    0.9128     0.9198    0.9128   0.9120   0.8791    0.8913   0.8791  0.8772\n",
            "KNN (euclidian)    0.7844     0.8071    0.7844   0.7829   0.7364    0.7666   0.7364  0.7330\n"
          ]
        }
      ],
      "source": [
        "# Tip: For cosine distance, brute-force search is commonly used.\n",
        "# Example (do not run until TODO 2/3 are done):\n",
        "#\n",
        "# knn_cos = KNeighborsClassifier(\n",
        "#     n_neighbors=7,\n",
        "#     metric=\"cosine\",\n",
        "#     algorithm=\"brute\",\n",
        "#     n_jobs=-1\n",
        "# )\n",
        "models = {\n",
        "    \"KNN (euclidian)\": KNeighborsClassifier(n_neighbors=7, metric=\"euclidean\", n_jobs=-1),\n",
        "    \"KNN (cos)\": KNeighborsClassifier(n_neighbors=7, metric=\"cosine\", algorithm=\"brute\", n_jobs=-1)\n",
        "}\n",
        "results = []\n",
        "best = {\"name\": None, \"pipe\": None, \"test_f1\": -1.0}\n",
        "\n",
        "for name, clf in models.items():\n",
        "    pipe = Pipeline(steps=[\n",
        "        (\"Vectorizer\", vectorizer),\n",
        "        (\"Classifier\", clf),\n",
        "    ])\n",
        "\n",
        "    pipe.fit(X_train_text, y_train)\n",
        "\n",
        "    yhat_tr = pipe.predict(X_train_text)\n",
        "    yhat_te = pipe.predict(X_test_text)\n",
        "\n",
        "    tr = weighted_metrics(y_train, yhat_tr)\n",
        "    te = weighted_metrics(y_test, yhat_te)\n",
        "\n",
        "    results.append([name, *tr, *te])\n",
        "\n",
        "    if te[3] > best[\"test_f1\"]:\n",
        "        best[\"name\"] = name\n",
        "        best[\"pipe\"] = pipe\n",
        "        best[\"test_f1\"] = te[3]\n",
        "\n",
        "cols = [\n",
        "    \"Model\",\n",
        "    \"Train Acc\", \"Train Prec\", \"Train Rec\", \"Train F1\",\n",
        "    \"Test Acc\", \"Test Prec\", \"Test Rec\", \"Test F1\",\n",
        "]\n",
        "\n",
        "out = pd.DataFrame(results, columns=cols).sort_values(\"Test F1\", ascending=False).reset_index(drop=True)\n",
        "\n",
        "pd.set_option(\"display.max_colwidth\", 80)\n",
        "print(\"\\n=== Results (sorted by Test F1) ===\")\n",
        "print(out.to_string(index=False, formatters={\n",
        "    \"Train Acc\": \"{:.4f}\".format,\n",
        "    \"Train Prec\": \"{:.4f}\".format,\n",
        "    \"Train Rec\": \"{:.4f}\".format,\n",
        "    \"Train F1\": \"{:.4f}\".format,\n",
        "    \"Test Acc\": \"{:.4f}\".format,\n",
        "    \"Test Prec\": \"{:.4f}\".format,\n",
        "    \"Test Rec\": \"{:.4f}\".format,\n",
        "    \"Test F1\": \"{:.4f}\".format,\n",
        "}))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ace5e4ac",
      "metadata": {
        "id": "ace5e4ac"
      },
      "source": [
        "## Discussion questions (answer in your own words)\n",
        "\n",
        "Write short answers below (2–5 sentences each is enough).\n",
        "\n",
        "### Question A\n",
        "In your own words, what is the added value of allowing 2-word sequences (bigrams) in `ngram_range`?\n",
        "\n",
        "### Question B\n",
        "In your own words, why might someone choose to set both `min_df` and `max_df` when building the vocabulary?\n",
        "\n",
        "### Question C\n",
        "\n",
        "After you run KNN with **Euclidean** and then with **Cosine** distance:\n",
        "\n",
        "- Do you observe any difference in results?\n",
        "- If yes, why do you think the difference happens (your intuition)?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d04ee569",
      "metadata": {
        "id": "d04ee569"
      },
      "source": [
        "**Your answers:**\n",
        "\n",
        "- **A:**  \n",
        "  *Allowing 2-word sequences can push our models to capture recurring 2-word sequences, which may be helpful for classifying\n",
        "a news article as fake news or real news. If fake news tend to have go-to/popular bigrams, our model will be able to pick\n",
        "this up and use this information to classify. If the bigram has significant weight, it may be enough to render news\n",
        "articles as fake! Essentially, allowing 2-word sequences enables our models to capture bigram trends in fake-news, which may be a significant factor when classifying news.*\n",
        "\n",
        "- **B:**  \n",
        "  *The parameters min_df and max_df are great tools for filtering words that are \"outliers\". In our data, we want to exclude words that are essential for effective English communication (such as: the, he, she, it, too, etc), since these words don't reveal information about the authenticity of news, rather assist in the articulation of ideas. For this reason, max_df will filter out words that are very common. Similarly, we want to exclude words that occur very few times since their scarcity will pose a challenge for the model to generalize from them. Min_df will ensure that the model does not overfit and limit the feature space for these models.*\n",
        "\n",
        "- **C:**  \n",
        "  *There is a significant difference in the evalutation metrics of both KNN models. The cosine model reported better accuracy across all evaluation metrics, suggesting a better fit for the data than the euclidian distance model. I believe that the cosine model was more successful in capturing the trends in the data and generalizing them because of the strucutre of the text vectors. For two text vectors with a huge difference in size, the euclidian model will infer a huge distance between them, whereas the cosine model will look past the size differences and focus the comparison on word usage patterns. Since we are more concerned with word trends and not size of documents, the cosine model is the more effective tool for comparing two text vectors, leading to better results for classification.*\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
